{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    init = tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bias(shape):\n",
    "    init = tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x,W):\n",
    "    # x ----> [batch , height , width , channels]\n",
    "    # w ----> [filter height , filter width , channel in , channel out]\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1] , padding = 'SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_2b2(x):\n",
    "    # x ----> [batch , height , width , channels]\n",
    "    return tf.nn.max_pool(x , ksize = [1,2,2,1] , strides= [1,2,2,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(input_x , shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input_x,W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_conn_layer(inp_layer , size):\n",
    "    inp_size = int(inp_layer.get_shape()[1])\n",
    "    W = init_weights([inp_size,size])\n",
    "    B = init_bias([size])\n",
    "    return tf.matmul(inp_layer,W) + B   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32 , shape=[None,784])\n",
    "Y_true = tf.placeholder(tf.float32 , shape=[None,10])\n",
    "x_img = tf.reshape(X , [-1,28,28,1])\n",
    "\n",
    "conv1 = conv_layer(x_img , shape = [5,5,1,32])\n",
    "conv1_pool = max_pool_2b2(conv1)\n",
    "\n",
    "conv2 = conv_layer(conv1_pool , shape = [5,5,32,64])\n",
    "conv2_pool = max_pool_2b2(conv2)\n",
    "\n",
    "conv3 = conv_layer(conv2_pool , shape = [5,5,64,32])\n",
    "conv3_pool = max_pool_2b2(conv3)\n",
    "\n",
    "conv_flat = tf.reshape(conv3_pool,[-1,4*4*32])\n",
    "fcn = tf.nn.relu(full_conn_layer(conv_flat , 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tensor.get_shape of <tf.Tensor 'Relu_52:0' shape=(?, 1024) dtype=float32>>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcn.get_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_prob = tf.placeholder(tf.float32)\n",
    "dropout = tf.nn.dropout(fcn , keep_prob=hold_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = full_conn_layer(dropout,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOSS\n",
    "cross_entr = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y_true , logits=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = opt.minimize(cross_entr)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   0.1405\n",
      "2   0.1726\n",
      "3   0.1718\n",
      "4   0.252\n",
      "5   0.2945\n",
      "6   0.2502\n",
      "7   0.234\n",
      "8   0.2447\n",
      "9   0.2924\n",
      "11   0.2804\n",
      "12   0.3132\n",
      "13   0.3883\n",
      "14   0.4661\n",
      "15   0.4974\n",
      "16   0.487\n",
      "17   0.4649\n",
      "18   0.4633\n",
      "19   0.4812\n",
      "21   0.5563\n",
      "22   0.5746\n",
      "23   0.6096\n",
      "24   0.6505\n",
      "25   0.6757\n",
      "26   0.6936\n",
      "27   0.7132\n",
      "28   0.7279\n",
      "29   0.731\n",
      "31   0.736\n",
      "32   0.7486\n",
      "33   0.7581\n",
      "34   0.7739\n",
      "35   0.7788\n",
      "36   0.7781\n",
      "37   0.7818\n",
      "38   0.7887\n",
      "39   0.8034\n",
      "41   0.8194\n",
      "42   0.8187\n",
      "43   0.815\n",
      "44   0.8148\n",
      "45   0.8043\n",
      "46   0.8017\n",
      "47   0.8132\n",
      "48   0.836\n",
      "49   0.8492\n",
      "51   0.8568\n",
      "52   0.8467\n",
      "53   0.8436\n",
      "54   0.8536\n",
      "55   0.8641\n",
      "56   0.8669\n",
      "57   0.8702\n",
      "58   0.8728\n",
      "59   0.8794\n",
      "61   0.8768\n",
      "62   0.88\n",
      "63   0.8759\n",
      "64   0.8631\n",
      "65   0.8717\n",
      "66   0.8784\n",
      "67   0.8847\n",
      "68   0.8865\n",
      "69   0.881\n",
      "71   0.8854\n",
      "72   0.8887\n",
      "73   0.8887\n",
      "74   0.8929\n",
      "75   0.8984\n",
      "76   0.906\n",
      "77   0.9084\n",
      "78   0.91\n",
      "79   0.9139\n",
      "81   0.9136\n",
      "82   0.9064\n",
      "83   0.901\n",
      "84   0.9015\n",
      "85   0.9056\n",
      "86   0.9073\n",
      "87   0.9074\n",
      "88   0.9056\n",
      "89   0.9068\n",
      "91   0.9165\n",
      "92   0.9204\n",
      "93   0.925\n",
      "94   0.9249\n",
      "95   0.9257\n",
      "96   0.9249\n",
      "97   0.9226\n",
      "98   0.9196\n",
      "99   0.9203\n",
      "101   0.9199\n",
      "102   0.9197\n",
      "103   0.9239\n",
      "104   0.9314\n",
      "105   0.935\n",
      "106   0.9332\n",
      "107   0.93\n",
      "108   0.9276\n",
      "109   0.9257\n",
      "111   0.9269\n",
      "112   0.9258\n",
      "113   0.9295\n",
      "114   0.9275\n",
      "115   0.9239\n",
      "116   0.9217\n",
      "117   0.922\n",
      "118   0.922\n",
      "119   0.9269\n",
      "121   0.9206\n",
      "122   0.9114\n",
      "123   0.9034\n",
      "124   0.9114\n",
      "125   0.9238\n",
      "126   0.9304\n",
      "127   0.9303\n",
      "128   0.9286\n",
      "129   0.9268\n",
      "131   0.9239\n",
      "132   0.9226\n",
      "133   0.9267\n",
      "134   0.9323\n",
      "135   0.936\n",
      "136   0.9328\n",
      "137   0.9265\n",
      "138   0.9258\n",
      "139   0.924\n",
      "141   0.9325\n",
      "142   0.937\n",
      "143   0.9403\n",
      "144   0.9379\n",
      "145   0.9379\n",
      "146   0.9386\n",
      "147   0.9391\n",
      "148   0.9383\n",
      "149   0.9368\n",
      "151   0.9374\n",
      "152   0.9382\n",
      "153   0.9404\n",
      "154   0.9405\n",
      "155   0.941\n",
      "156   0.9404\n",
      "157   0.9445\n",
      "158   0.944\n",
      "159   0.9383\n",
      "161   0.9287\n",
      "162   0.937\n",
      "163   0.9417\n",
      "164   0.9374\n",
      "165   0.9345\n",
      "166   0.9334\n",
      "167   0.939\n",
      "168   0.9421\n",
      "169   0.9429\n",
      "171   0.9427\n",
      "172   0.9428\n",
      "173   0.9405\n",
      "174   0.939\n",
      "175   0.9415\n",
      "176   0.9442\n",
      "177   0.9468\n",
      "178   0.9489\n",
      "179   0.9485\n",
      "181   0.9436\n",
      "182   0.9432\n",
      "183   0.9446\n",
      "184   0.9473\n",
      "185   0.9511\n",
      "186   0.952\n",
      "187   0.9502\n",
      "188   0.9482\n",
      "189   0.9459\n",
      "191   0.9428\n",
      "192   0.9449\n",
      "193   0.9436\n",
      "194   0.9425\n",
      "195   0.9435\n",
      "196   0.9438\n",
      "197   0.9459\n",
      "198   0.95\n",
      "199   0.9516\n",
      "201   0.9502\n",
      "202   0.9475\n",
      "203   0.945\n",
      "204   0.9439\n",
      "205   0.9457\n",
      "206   0.9498\n",
      "207   0.9473\n",
      "208   0.9433\n",
      "209   0.9383\n",
      "211   0.9393\n",
      "212   0.9418\n",
      "213   0.9462\n",
      "214   0.948\n",
      "215   0.9505\n",
      "216   0.9519\n",
      "217   0.9532\n",
      "218   0.9544\n",
      "219   0.9528\n",
      "221   0.9423\n",
      "222   0.9425\n",
      "223   0.9445\n",
      "224   0.9468\n",
      "225   0.9451\n",
      "226   0.9417\n",
      "227   0.9421\n",
      "228   0.943\n",
      "229   0.9437\n",
      "231   0.9457\n",
      "232   0.9485\n",
      "233   0.951\n",
      "234   0.9504\n",
      "235   0.9509\n",
      "236   0.9525\n",
      "237   0.9529\n",
      "238   0.9508\n",
      "239   0.9521\n",
      "241   0.9516\n",
      "242   0.952\n",
      "243   0.9549\n",
      "244   0.9562\n",
      "245   0.9559\n",
      "246   0.9532\n",
      "247   0.9493\n",
      "248   0.9468\n",
      "249   0.9444\n",
      "251   0.9471\n",
      "252   0.9472\n",
      "253   0.9453\n",
      "254   0.9421\n",
      "255   0.9412\n",
      "256   0.9431\n",
      "257   0.9476\n",
      "258   0.9538\n",
      "259   0.9575\n",
      "261   0.9611\n",
      "262   0.9608\n",
      "263   0.961\n",
      "264   0.9602\n",
      "265   0.9591\n",
      "266   0.9593\n",
      "267   0.9564\n",
      "268   0.9548\n",
      "269   0.9556\n",
      "271   0.9579\n",
      "272   0.9596\n",
      "273   0.9587\n",
      "274   0.9559\n",
      "275   0.9526\n",
      "276   0.9506\n",
      "277   0.9489\n",
      "278   0.951\n",
      "279   0.9562\n",
      "281   0.9589\n",
      "282   0.9595\n",
      "283   0.9602\n",
      "284   0.9607\n",
      "285   0.9612\n",
      "286   0.9601\n",
      "287   0.9596\n",
      "288   0.9596\n",
      "289   0.9611\n",
      "291   0.9631\n",
      "292   0.9616\n",
      "293   0.9579\n",
      "294   0.9529\n",
      "295   0.9524\n",
      "296   0.9531\n",
      "297   0.9555\n",
      "298   0.9561\n",
      "299   0.9534\n",
      "301   0.9608\n",
      "302   0.9618\n",
      "303   0.9616\n",
      "304   0.9609\n",
      "305   0.9605\n",
      "306   0.9582\n",
      "307   0.955\n",
      "308   0.9525\n",
      "309   0.9497\n",
      "311   0.9553\n",
      "312   0.9593\n",
      "313   0.9597\n",
      "314   0.9613\n",
      "315   0.9593\n",
      "316   0.9575\n",
      "317   0.9574\n",
      "318   0.9567\n",
      "319   0.9554\n",
      "321   0.9562\n",
      "322   0.9579\n",
      "323   0.9593\n",
      "324   0.9606\n",
      "325   0.9614\n",
      "326   0.96\n",
      "327   0.9587\n",
      "328   0.9595\n",
      "329   0.9612\n",
      "331   0.9632\n",
      "332   0.9665\n",
      "333   0.9664\n",
      "334   0.9667\n",
      "335   0.9672\n",
      "336   0.9654\n",
      "337   0.9654\n",
      "338   0.9647\n",
      "339   0.964\n",
      "341   0.963\n",
      "342   0.962\n",
      "343   0.9596\n",
      "344   0.9593\n",
      "345   0.9597\n",
      "346   0.9621\n",
      "347   0.9616\n",
      "348   0.9598\n",
      "349   0.9598\n",
      "351   0.9547\n",
      "352   0.9511\n",
      "353   0.9524\n",
      "354   0.9563\n",
      "355   0.9608\n",
      "356   0.9629\n",
      "357   0.9602\n",
      "358   0.9544\n",
      "359   0.9476\n",
      "361   0.948\n",
      "362   0.9533\n",
      "363   0.9589\n",
      "364   0.9632\n",
      "365   0.9656\n",
      "366   0.9655\n",
      "367   0.9639\n",
      "368   0.9596\n",
      "369   0.9579\n",
      "371   0.9525\n",
      "372   0.9527\n",
      "373   0.957\n",
      "374   0.9602\n",
      "375   0.9621\n",
      "376   0.9611\n",
      "377   0.9612\n",
      "378   0.96\n",
      "379   0.9591\n",
      "381   0.9577\n",
      "382   0.9608\n",
      "383   0.9641\n",
      "384   0.9659\n",
      "385   0.9662\n",
      "386   0.9659\n",
      "387   0.9657\n",
      "388   0.9659\n",
      "389   0.9658\n",
      "391   0.9656\n",
      "392   0.9652\n",
      "393   0.9639\n",
      "394   0.964\n",
      "395   0.9646\n",
      "396   0.9642\n",
      "397   0.9641\n",
      "398   0.9633\n",
      "399   0.9632\n",
      "401   0.9645\n",
      "402   0.9655\n",
      "403   0.9651\n",
      "404   0.9662\n",
      "405   0.9659\n",
      "406   0.965\n",
      "407   0.9637\n",
      "408   0.9622\n",
      "409   0.9591\n",
      "411   0.9576\n",
      "412   0.9582\n",
      "413   0.9598\n",
      "414   0.963\n",
      "415   0.9666\n",
      "416   0.9673\n",
      "417   0.9677\n",
      "418   0.9642\n",
      "419   0.9604\n",
      "421   0.9536\n",
      "422   0.9545\n",
      "423   0.9586\n",
      "424   0.9614\n",
      "425   0.9648\n",
      "426   0.9676\n",
      "427   0.9683\n",
      "428   0.9691\n",
      "429   0.9697\n",
      "431   0.9712\n",
      "432   0.9698\n",
      "433   0.9684\n",
      "434   0.9672\n",
      "435   0.9674\n",
      "436   0.9677\n",
      "437   0.9689\n",
      "438   0.9688\n",
      "439   0.9699\n",
      "441   0.9677\n",
      "442   0.9667\n",
      "443   0.9689\n",
      "444   0.9703\n",
      "445   0.9704\n",
      "446   0.9693\n",
      "447   0.9697\n",
      "448   0.9694\n",
      "449   0.9693\n",
      "451   0.9713\n",
      "452   0.9714\n",
      "453   0.9724\n",
      "454   0.9726\n",
      "455   0.9719\n",
      "456   0.9717\n",
      "457   0.9706\n",
      "458   0.9679\n",
      "459   0.9684\n",
      "461   0.9692\n",
      "462   0.9695\n",
      "463   0.9703\n",
      "464   0.9705\n",
      "465   0.9699\n",
      "466   0.9717\n",
      "467   0.9731\n",
      "468   0.9723\n",
      "469   0.9708\n",
      "471   0.9673\n",
      "472   0.9653\n",
      "473   0.9625\n",
      "474   0.961\n",
      "475   0.9663\n",
      "476   0.971\n",
      "477   0.9727\n",
      "478   0.973\n",
      "479   0.9711\n",
      "481   0.9668\n",
      "482   0.9648\n",
      "483   0.9639\n",
      "484   0.9628\n",
      "485   0.9629\n",
      "486   0.9656\n",
      "487   0.9669\n",
      "488   0.9672\n",
      "489   0.9665\n",
      "491   0.9637\n",
      "492   0.9625\n",
      "493   0.9626\n",
      "494   0.9608\n",
      "495   0.9612\n",
      "496   0.9663\n",
      "497   0.9694\n",
      "498   0.9708\n",
      "499   0.9695\n"
     ]
    }
   ],
   "source": [
    "import tqdm \n",
    "epochs = 500\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in (range(epochs)):\n",
    "        \n",
    "        batch_x , batch_y = mnist.train.next_batch(50)\n",
    "        sess.run(train , feed_dict = {X:batch_x , Y_true : batch_y , hold_prob:0.5})\n",
    "        \n",
    "        if i%10:\n",
    "            \n",
    "            match = tf.equal(tf.argmax(y_pred,1) , tf.argmax(Y_true , 1))\n",
    "            \n",
    "            acc = tf.reduce_mean(tf.cast(match,tf.float32))\n",
    "            \n",
    "            print(str(i) + \"   \" + str(sess.run(acc,feed_dict = {X:mnist.test.images , Y_true : mnist.test.labels , hold_prob : 1.0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
